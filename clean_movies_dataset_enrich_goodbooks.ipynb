{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Movies Dataset Cleanup Utility and Book Summaries Fuzzy Matcher\n",
    "#@author Velizar Petrov\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "\n",
    "# 1. Load datasets\n",
    "movies = pd.read_csv(\"Datasets\\\\The Movies Dataset\\\\movies_metadata.csv\", low_memory=False)\n",
    "ratings = pd.read_csv(\"Datasets\\\\The Movies Dataset\\\\ratings.csv\")\n",
    "keywords = pd.read_csv(\"Datasets\\\\The Movies Dataset\\\\keywords.csv\")\n",
    "credits = pd.read_csv(\"Datasets\\\\The Movies Dataset\\\\credits.csv\")\n",
    "\n",
    "\n",
    "# 2. Clean movies_metadata\n",
    "movies['id'] = pd.to_numeric(movies['id'], errors='coerce')\n",
    "movies = movies.dropna(subset=['id'])\n",
    "movies['id'] = movies['id'].astype(int)\n",
    "\n",
    "# Drop duplicate IDs and titles\n",
    "movies = movies.drop_duplicates(subset='id')\n",
    "movies = movies.drop_duplicates(subset=['title', 'release_date'])\n",
    "\n",
    "\n",
    "# 3. Clean ratings\n",
    "ratings['movieId'] = pd.to_numeric(ratings['movieId'], errors='coerce')\n",
    "ratings = ratings.dropna(subset=['movieId'])\n",
    "ratings['movieId'] = ratings['movieId'].astype(int)\n",
    "\n",
    "# Drop duplicate (userId, movieId)\n",
    "ratings = ratings.drop_duplicates(subset=['userId', 'movieId'])\n",
    "\n",
    "# Align with movies\n",
    "ratings = ratings[ratings['movieId'].isin(movies['id'])]\n",
    "\n",
    "\n",
    "# 4. Clean and parse keywords\n",
    "keywords['id'] = pd.to_numeric(keywords['id'], errors='coerce')\n",
    "keywords = keywords.dropna(subset=['id'])\n",
    "keywords['id'] = keywords['id'].astype(int)\n",
    "keywords = keywords.drop_duplicates(subset='id')\n",
    "\n",
    "# Parse keyword list\n",
    "def parse_keywords(x):\n",
    "    try:\n",
    "        kws = ast.literal_eval(x)\n",
    "        return [d['name'] for d in kws] if isinstance(kws, list) else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "keywords['keywords'] = keywords['keywords'].apply(parse_keywords)\n",
    "\n",
    "\n",
    "# 5. Clean and parse credits\n",
    "\n",
    "credits['id'] = pd.to_numeric(credits['id'], errors='coerce')\n",
    "credits = credits.dropna(subset=['id'])\n",
    "credits['id'] = credits['id'].astype(int)\n",
    "credits = credits.drop_duplicates(subset='id')\n",
    "\n",
    "# Parse JSON-like strings\n",
    "def parse_cast(x, top_n=3):\n",
    "    try:\n",
    "        cast = ast.literal_eval(x)\n",
    "        return [d['name'] for d in cast[:top_n]] if isinstance(cast, list) else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def parse_director(x):\n",
    "    try:\n",
    "        crew = ast.literal_eval(x)\n",
    "        directors = [d['name'] for d in crew if d.get('job') == 'Director']\n",
    "        return directors[:1] if directors else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "credits['cast'] = credits['cast'].apply(parse_cast)\n",
    "credits['director'] = credits['crew'].apply(parse_director)\n",
    "credits = credits.drop(columns=['crew'])  # drop raw crew\n",
    "\n",
    "# 6. Merge all content features\n",
    "\n",
    "# Keep genres from movies\n",
    "def parse_genres(x):\n",
    "    try:\n",
    "        g = ast.literal_eval(x)\n",
    "        return [d['name'] for d in g] if isinstance(g, list) else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "movies['genres'] = movies['genres'].apply(parse_genres)\n",
    "\n",
    "# Merge with keywords and credits\n",
    "content = movies[['id', 'title', 'genres']].merge(\n",
    "    keywords[['id', 'keywords']], on='id', how='left'\n",
    ").merge(\n",
    "    credits[['id', 'cast', 'director']], on='id', how='left'\n",
    ")\n",
    "\n",
    "# Fill missing lists\n",
    "for col in ['genres', 'keywords', 'cast', 'director']:\n",
    "    content[col] = content[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Build combined feature string\n",
    "content['content_features'] = content['genres'] + content['keywords'] + content['cast'] + content['director']\n",
    "content['content_features'] = content['content_features'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "\n",
    "# 7. Save cleaned datasets\n",
    "movies.to_csv(\"movies_clean.csv\", index=False)\n",
    "ratings.to_csv(\"ratings_clean.csv\", index=False)\n",
    "content[['id', 'title', 'content_features']].to_csv(\"content_features.csv\", index=False)\n",
    "\n",
    "print(\"  Cleaning complete! Saved:\")\n",
    "print(\"- movies_clean.csv\")\n",
    "print(\"- ratings_clean.csv\")\n",
    "print(\"- content_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ca3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "goodbooks = pd.read_csv(\"goodbooks_10k_rating_and_description.csv\")\n",
    "summaries = pd.read_csv(\"booksummaries.csv\")\n",
    "\n",
    "# Normalize titles\n",
    "goodbooks['book_title_clean'] = goodbooks['book_title'].str.strip().str.lower()\n",
    "summaries['title_clean'] = summaries['title'].str.strip().str.lower()\n",
    "\n",
    "# Create lookup dictionary\n",
    "desc_dict = summaries.set_index('title_clean')['description'].to_dict()\n",
    "\n",
    "# Reporting variables\n",
    "initial_missing = goodbooks['book_desc'].isna().sum()\n",
    "already_filled = goodbooks['book_desc'].notna().sum()\n",
    "\n",
    "# Exact match pass \n",
    "exact_matches = 0\n",
    "def exact_match(row):\n",
    "    global exact_matches\n",
    "    if pd.isna(row['book_desc']):\n",
    "        desc = desc_dict.get(row['book_title_clean'])\n",
    "        if desc:\n",
    "            exact_matches += 1\n",
    "            return desc\n",
    "    return row['book_desc']\n",
    "\n",
    "goodbooks['book_desc'] = goodbooks.apply(exact_match, axis=1)\n",
    "\n",
    "# Fuzzy match pass \n",
    "fuzzy_matches = 0\n",
    "title_list = summaries['title_clean'].tolist()\n",
    "missing_mask = goodbooks['book_desc'].isna()\n",
    "title_to_desc = desc_dict\n",
    "\n",
    "def fuzzy_fill(title):\n",
    "    global fuzzy_matches\n",
    "    match = process.extractOne(title, title_list, scorer=fuzz.token_sort_ratio, score_cutoff=90)\n",
    "    if match:\n",
    "        fuzzy_matches += 1\n",
    "        return title_to_desc.get(match[0])\n",
    "    return None\n",
    "\n",
    "goodbooks.loc[missing_mask, 'book_desc'] = goodbooks.loc[missing_mask, 'book_title_clean'].apply(fuzzy_fill)\n",
    "\n",
    "# Final reporting \n",
    "final_missing = goodbooks['book_desc'].isna().sum()\n",
    "final_filled = goodbooks['book_desc'].notna().sum()\n",
    "\n",
    "print(\"=== Matching Report ===\")\n",
    "print(f\"Total books: {len(goodbooks)}\")\n",
    "print(f\"Descriptions initially filled: {already_filled}\")\n",
    "print(f\"Filled via exact title match: {exact_matches}\")\n",
    "print(f\"Filled via fuzzy title match: {fuzzy_matches}\")\n",
    "print(f\"Still missing: {final_missing}\")\n",
    "\n",
    "# Save result\n",
    "goodbooks.drop(columns=['book_title_clean'], inplace=True)\n",
    "goodbooks.to_csv(\"goodbooks_10k_rating_and_description_fuzzy_filled.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
